# -*- coding: utf-8 -*-
"""pointwise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bo8IzEhxgmLC_uXazh_WqH_ghZgOLPMF

### Install Dependencies
"""


"""### Load Data

#### Import libraries
"""

import gc
import random
import os

import pandas as pd
import numpy as np
from sklearn.utils import class_weight
from sklearn.model_selection import train_test_split
import transformers
from transformers import BertTokenizer, TFBertModel, BertConfig
import tensorflow as tf
import tensorflow_addons as tfa

random.seed(0)
np.random.seed(0)
tf.random.set_seed(0)

"""#### Read data"""

root_dir = "./"

qrels = pd.read_csv('qrels.dev.tsv',sep='\t',header=None)

qrels.head(5)

top1000 = pd.read_csv('top1000.dev',sep='\t',header=None)

top1000 = top1000.sort_values(by=0).reset_index(drop=True)

"""### Preprocessing

Set train set size (number of query passage pairs)
"""

bert_model_name = 'bert-base-uncased'
passage_per_query = 50
num_queries = 6000

tokenizer = BertTokenizer.from_pretrained('./bert-tokenizer/')

qrels = qrels[[0,2]].rename(columns={0: 0, 2: 1})

qrels['y']=1

"""Train Test Split"""

qrels = qrels[qrels[0].isin(top1000[0].unique()[:num_queries])]

split = np.random.choice(qrels[0].unique(),qrels.shape[0]//5)
train_queries = qrels[~qrels[0].isin(split)]
test_queries = qrels[qrels[0].isin(split)]

assert not train_queries[0].isin(test_queries[0]).any()

"""Add labels"""

def generateTable(queries):
  # Add labels
  df = pd.merge(top1000, queries,  how='left', left_on=[0,1], right_on = [0,1]).fillna(0)
  # drop queries not in set
  df = df[df[0].isin(queries[0])][[2,3,'y']]
  # positive samples
  df_p = df[df['y']==1].reset_index(drop=True)
  # negative samples
  df_n = df[(df[2].isin(df_p[2])) & (df['y']==0)]
  # sample
  df_n = df_n.groupby(2).sample(n=passage_per_query,replace=True).drop_duplicates().reset_index(drop=True)
  # append and shuffle
  df = df_p.append(df_n, ignore_index=True).sample(frac=1).reset_index(drop=True)
  return df

train_set = generateTable(train_queries)
test_set = generateTable(test_queries)

"""There shouldn't be any overlap in queries"""

assert not train_set[2].isin(test_set[2]).any()

# from sklearn.model_selection import train_test_split
# train_set,test_set = train_test_split(train_set,test_size=0.2)

# batch_size = 128
# def generateSamples(df):
#   for i,row in df.iterrows():
#     q = tokenizer(row[2], return_tensors="tf",padding="max_length",max_length=q_trunc,truncation=True)
#     p = tokenizer(row[3], return_tensors="tf",padding="max_length",max_length=p_trunc,truncation=True)
#     X={}
#     for id in ['input_ids','token_type_ids','attention_mask']:
#       X[id] = tf.concat([q[id],p[id][:,1:]],1)
#     yield X,[row['y']]
# type_dict = {id:tf.int32 for id in ['input_ids','token_type_ids','attention_mask']}
# train_ds = tf.data.Dataset.from_generator(lambda :generateSamples(train_set),(type_dict,tf.int32))
# train_ds = train_ds.batch(batch_size).prefetch(1)
# test_ds = tf.data.Dataset.from_generator(lambda :generateSamples(test_set),(type_dict,tf.int32))
# test_ds = test_ds.batch(batch_size).prefetch(1)

"""##### Tokenize

Truncation Limits
"""

q_trunc = 64
p_trunc = 449
assert q_trunc+p_trunc-1==512

"""Create training set"""

def generateSet(df):
  # tokenize
  q = tokenizer(df[2].values.tolist(), return_tensors="tf",padding="max_length",max_length=q_trunc,truncation=True)
  p = tokenizer(df[3].values.tolist(), return_tensors="tf",padding="max_length",max_length=p_trunc,truncation=True)
  X=[]
  # labels
  y = df['y']
  for id in ['input_ids','token_type_ids','attention_mask']:
    X.append(tf.concat([q[id],p[id][:,1:]],1))
  # delete unwanted variables
  q = None
  p = None
  gc.collect()
  return X,y

X_train,y_train = generateSet(train_set)
X_test,y_test = generateSet(test_set)

"""#### Setup Tensorflow

Setup TPU
"""

# #Get a handle to the attached TPU. On GCP it will be the CloudTPU itself
# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
# #Connect to the TPU handle and initialise it
# tf.config.experimental_connect_to_cluster(resolver)
# tf.tpu.experimental.initialize_tpu_system(resolver)
# strategy = tf.distribute.experimental.TPUStrategy(resolver)

tf.config.run_functions_eagerly(False)

"""### Create Model

Batch size: 128 on TPU, 8 on GPU
"""

num_epochs = 4
batch_size = 8
num_train_steps = X_train[0].shape[0]//batch_size*num_epochs

"""A model with three inputs:
- Input IDs
- Token Type IDs
- Attention Masks
These are from tokenizer.

Fed to BERT, from which the [CLS] vector is taken, and fed into a dense layer with one output after sigmoid.
"""

def create_inputs(num_nodes,name):
  layers = []
  for layer_name in ['input_ids','token_type_ids','attention_mask']:
    layers.append(tf.keras.layers.Input(shape=(num_nodes,),dtype=tf.int32,name=layer_name))
  return layers

def create_model(output_bias=None):
    if output_bias is not None:
      output_bias = tf.keras.initializers.Constant(output_bias)
    config = BertConfig(hidden_dropout_prob=0.1)
    bert = TFBertModel.from_pretrained('./bert-model/',config=config)
    for layer in bert.layers[:]:
      if isinstance(layer, transformers.models.bert.modeling_tf_bert.TFBertMainLayer):
        layer.embeddings.trainable=False
        layer.pooler.trainable=False
        for idx, layer in enumerate(layer.encoder.layer):
            # print(layer)
            # freeze first 10
            if idx in range(10):
                layer.trainable = False
      else:
        layer.trainable = False
              
    input_layer = create_inputs(512,'pair')
    bert_out = bert(input_layer).last_hidden_state
    cls = tf.keras.layers.Lambda(lambda x:x[:,0,:])(bert_out)
    # print(avg_q.shape)
    output = tf.keras.layers.Dense(1, activation="sigmoid",bias_initializer=output_bias)(cls)
    model = tf.keras.models.Model(inputs=input_layer, outputs=[output])
    opt,schedule = transformers.create_optimizer(num_train_steps=num_train_steps,init_lr=3e-5,adam_beta1=0.9,adam_beta2=0.999,weight_decay_rate=0.01,num_warmup_steps=num_train_steps//20)
    # opt = tfa.optimizers.RectifiedAdam()
    model.compile(optimizer=opt,
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.AUC(curve="ROC")])
    model.summary()
    return model

"""Save on best validation loss"""

checkpoint = tf.keras.callbacks.ModelCheckpoint(root_dir + "pointwise_best_model.h5", monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min', save_freq='epoch')
es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)

"""Check GPU"""

tf.test.gpu_device_name()

"""Free memory"""

gc.collect()

"""Hide warnings"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 
tf.get_logger().setLevel('WARNING')

"""Biases, and class weights"""

total = len(train_set['y'])
pos = train_set['y'].sum()
neg = total-pos
initial_bias = np.log([pos/neg])

weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}
# class_weight={0:1,1:1}

"""#### Train"""

def createAndRun():
  model = None
  if initial_bias:
    print("Bias",initial_bias)
    model = create_model(initial_bias)
  else:
    model = create_model()
  gc.collect()
  print("Class weight",class_weight)
  # model.fit(x=train_ds, epochs=4, verbose=1, validation_data=test_ds, callbacks=[checkpoint])
  history = model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=num_epochs, verbose=2, validation_data=(X_test,y_test), use_multiprocessing=False, callbacks=[checkpoint,es],class_weight=class_weight)
  model.save_weights('pointwise_final_model.h5')
  return model,history

def cpu():
  return createAndRun()

def gpu():
  with tf.device('/device:GPU:0'):
    return createAndRun()

def tpu():
  with strategy.scope():
    return createAndRun()

# model,history = cpu()

model,history = gpu()

# model,history = tpu()

